#!/usr/bin/env python3
"""
BERTè®­ç»ƒæ•°æ®è½¬æ¢å·¥å…·

åŠŸèƒ½ï¼š
1. åˆå¹¶å¤šä¸ªæ–‡æ¡£çš„è®­ç»ƒæ•°æ®
2. ç”Ÿæˆæ ‡å‡†çš„BIOæ ‡æ³¨æ ¼å¼
3. åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†
4. è¾“å‡ºé€‚åˆBERTæ¨¡å‹çš„æ ¼å¼

ç‰ˆæœ¬ï¼š1.0
"""

import json
import os
import random
from pathlib import Path
from typing import List, Dict, Any, Tuple
from collections import defaultdict, Counter

class BERTDataConverter:
    def __init__(self):
        self.entity_types = ['ç—…å', 'ç—‡çŠ¶', 'ç©´ä½', 'æ²»æ³•']
        self.bio_tags = ['O'] + [f'B-{t}' for t in self.entity_types] + [f'I-{t}' for t in self.entity_types]
        
    def load_training_data(self, data_dir: str = "outputs/training_data") -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰è®­ç»ƒæ•°æ®æ–‡ä»¶"""
        training_files = []
        data_path = Path(data_dir)
        
        if not data_path.exists():
            print(f"âŒ è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return []
        
        # æŸ¥æ‰¾æ‰€æœ‰bert_training_*.jsonæ–‡ä»¶
        for file_path in data_path.glob("bert_training_*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    training_files.append(data)
                    print(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®: {file_path.name}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {file_path.name}: {e}")
        
        return training_files
    
    def convert_to_bio_format(self, training_data: List[Dict[str, Any]]) -> List[Tuple[List[str], List[str]]]:
        """è½¬æ¢ä¸ºBIOæ ¼å¼"""
        bio_data = []
        
        for doc_data in training_data:
            doc_name = doc_data.get('doc_name', 'unknown')
            print(f"ğŸ”„ å¤„ç†æ–‡æ¡£: {doc_name}")
            
            sentences = doc_data.get('sentences', [])
            for sentence_data in sentences:
                text = sentence_data.get('text', '')
                entities = sentence_data.get('entities', [])
                
                if not text.strip():
                    continue
                
                # åˆå§‹åŒ–æ ‡ç­¾
                chars = list(text)
                labels = ['O'] * len(chars)
                
                # æ ‡æ³¨å®ä½“
                for entity in entities:
                    entity_text = entity.get('text', '')
                    entity_type = entity.get('type', '')
                    
                    if entity_type not in self.entity_types:
                        continue
                    
                    # æŸ¥æ‰¾å®ä½“åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
                    start_pos = text.find(entity_text)
                    if start_pos != -1:
                        end_pos = start_pos + len(entity_text)
                        
                        # BIOæ ‡æ³¨
                        for i in range(start_pos, end_pos):
                            if i == start_pos:
                                labels[i] = f'B-{entity_type}'
                            else:
                                labels[i] = f'I-{entity_type}'
                
                bio_data.append((chars, labels))
        
        return bio_data
    
    def split_dataset(self, bio_data: List[Tuple[List[str], List[str]]], 
                     train_ratio: float = 0.7, val_ratio: float = 0.15) -> Tuple[List, List, List]:
        """åˆ’åˆ†æ•°æ®é›†"""
        random.shuffle(bio_data)
        
        total_size = len(bio_data)
        train_size = int(total_size * train_ratio)
        val_size = int(total_size * val_ratio)
        
        train_data = bio_data[:train_size]
        val_data = bio_data[train_size:train_size + val_size]
        test_data = bio_data[train_size + val_size:]
        
        print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
        print(f"   è®­ç»ƒé›†: {len(train_data)} ä¸ªå¥å­")
        print(f"   éªŒè¯é›†: {len(val_data)} ä¸ªå¥å­")
        print(f"   æµ‹è¯•é›†: {len(test_data)} ä¸ªå¥å­")
        
        return train_data, val_data, test_data
    
    def save_bio_file(self, data: List[Tuple[List[str], List[str]]], file_path: str):
        """ä¿å­˜BIOæ ¼å¼æ–‡ä»¶"""
        with open(file_path, 'w', encoding='utf-8') as f:
            for chars, labels in data:
                for char, label in zip(chars, labels):
                    f.write(f"{char}\t{label}\n")
                f.write("\n")  # å¥å­é—´ç©ºè¡Œ
    
    def generate_config(self, output_dir: str, stats: Dict[str, Any]):
        """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
        config = {
            "model_type": "bert-bilstm-crf",
            "entity_types": self.entity_types,
            "bio_tags": self.bio_tags,
            "max_seq_length": stats.get('max_length', 512),
            "train_file": "train.txt",
            "val_file": "val.txt", 
            "test_file": "test.txt",
            "dataset_stats": stats
        }
        
        config_path = os.path.join(output_dir, "config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}")
    
    def generate_readme(self, output_dir: str, stats: Dict[str, Any]):
        """ç”ŸæˆREADMEæ–‡æ¡£"""
        readme_content = f"""# BERTè®­ç»ƒæ•°æ®åŒ…

## ğŸ“Š æ•°æ®é›†ä¿¡æ¯
- æ€»æ–‡æ¡£æ•°: {stats.get('total_docs', 0)}
- è®­ç»ƒé›†: {stats.get('train_sentences', 0)} ä¸ªå¥å­
- éªŒè¯é›†: {stats.get('val_sentences', 0)} ä¸ªå¥å­  
- æµ‹è¯•é›†: {stats.get('test_sentences', 0)} ä¸ªå¥å­
- æœ€å¤§åºåˆ—é•¿åº¦: {stats.get('max_length', 0)}

## ğŸ·ï¸ å®ä½“ç±»å‹
{chr(10).join([f"- {entity_type}: {stats.get('entity_counts', {}).get(entity_type, 0)} ä¸ªå®ä½“" for entity_type in self.entity_types])}

## ğŸ“ æ–‡ä»¶è¯´æ˜
- `train.txt`: è®­ç»ƒé›†ï¼ˆBIOæ ¼å¼ï¼‰
- `val.txt`: éªŒè¯é›†ï¼ˆBIOæ ¼å¼ï¼‰
- `test.txt`: æµ‹è¯•é›†ï¼ˆBIOæ ¼å¼ï¼‰
- `config.json`: æ¨¡å‹é…ç½®æ–‡ä»¶
- `README.md`: æœ¬è¯´æ˜æ–‡æ¡£

## ğŸ”§ ä½¿ç”¨æ–¹æ³•
1. ä½¿ç”¨BERTä½œä¸ºç¼–ç å™¨
2. æ·»åŠ BiLSTMå±‚è¿›è¡Œåºåˆ—å»ºæ¨¡
3. ä½¿ç”¨CRFå±‚è¿›è¡Œåºåˆ—æ ‡æ³¨
4. æ”¯æŒçš„å®ä½“ç±»å‹: {', '.join(self.entity_types)}

## ğŸ“ˆ æ•°æ®è´¨é‡
- æ•°æ®æ¥æº: å¤šæ¨¡å‹ååŒæŠ½å– + äººå·¥å®¡æ ¸
- æ ‡æ³¨æ ¼å¼: BIOæ ‡å‡†æ ¼å¼
- è´¨é‡ä¿è¯: ä¸“å®¶å®¡æ ¸ç¡®è®¤
"""
        
        readme_path = os.path.join(output_dir, "README.md")
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print(f"âœ… è¯´æ˜æ–‡æ¡£å·²ä¿å­˜: {readme_path}")
    
    def calculate_stats(self, train_data: List, val_data: List, test_data: List, 
                       training_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        entity_counts = defaultdict(int)
        max_length = 0
        
        # ç»Ÿè®¡å®ä½“æ•°é‡å’Œæœ€å¤§é•¿åº¦
        for doc_data in training_data:
            for sentence_data in doc_data.get('sentences', []):
                text = sentence_data.get('text', '')
                max_length = max(max_length, len(text))
                
                for entity in sentence_data.get('entities', []):
                    entity_type = entity.get('type', '')
                    if entity_type in self.entity_types:
                        entity_counts[entity_type] += 1
        
        return {
            'total_docs': len(training_data),
            'train_sentences': len(train_data),
            'val_sentences': len(val_data),
            'test_sentences': len(test_data),
            'max_length': max_length,
            'entity_counts': dict(entity_counts)
        }
    
    def convert(self, output_dir: str = "outputs/training_data/bert_training_package"):
        """ä¸»è½¬æ¢å‡½æ•°"""
        print("ğŸš€ å¼€å§‹BERTè®­ç»ƒæ•°æ®è½¬æ¢...")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        training_data = self.load_training_data()
        if not training_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
            return
        
        # è½¬æ¢ä¸ºBIOæ ¼å¼
        bio_data = self.convert_to_bio_format(training_data)
        if not bio_data:
            print("âŒ è½¬æ¢BIOæ ¼å¼å¤±è´¥")
            return
        
        # åˆ’åˆ†æ•°æ®é›†
        train_data, val_data, test_data = self.split_dataset(bio_data)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ•°æ®æ–‡ä»¶
        self.save_bio_file(train_data, os.path.join(output_dir, "train.txt"))
        self.save_bio_file(val_data, os.path.join(output_dir, "val.txt"))
        self.save_bio_file(test_data, os.path.join(output_dir, "test.txt"))
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = self.calculate_stats(train_data, val_data, test_data, training_data)
        
        # ç”Ÿæˆé…ç½®å’Œæ–‡æ¡£
        self.generate_config(output_dir, stats)
        self.generate_readme(output_dir, stats)
        
        print(f"\nâœ… BERTè®­ç»ƒæ•°æ®è½¬æ¢å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“Š æ€»è®¡: {len(bio_data)} ä¸ªè®­ç»ƒæ ·æœ¬")

def main():
    """ä¸»å‡½æ•°"""
    converter = BERTDataConverter()
    converter.convert()

if __name__ == "__main__":
    main()
