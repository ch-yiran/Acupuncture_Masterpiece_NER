#!/usr/bin/env python3
"""
é’ˆç¸å¤§æˆé›†æˆå®ä½“æŠ½å–ç³»ç»Ÿ
åŒæ—¶è°ƒç”¨ä¸‰ä¸ªæ¨¡å‹è¿›è¡ŒæŠ½å–ï¼Œå¯¹æ¯”ä¸€è‡´æ€§ï¼Œæ”¯æŒäººå·¥å®¡æ ¸å’Œæ‰¹é‡å¤„ç†

æ¨¡å‹å¯é æ€§æ’åºï¼šè±†åŒ… > DeepSeek > é€šä¹‰åƒé—®

åŠŸèƒ½ï¼š
1. æ‰¹é‡è°ƒç”¨ä¸‰ä¸ªæ¨¡å‹æŠ½å–å™¨
2. æ™ºèƒ½å¯¹æ¯”å’Œä¸€è‡´æ€§åˆ†æ
3. äººå·¥å®¡æ ¸ç•Œé¢ï¼ˆå¯ç´¯ç§¯å®¡æ ¸ï¼‰
4. æ‰¹é‡å¤„ç†æ–‡æœ¬æ–‡ä»¶
5. ç»“æœç»Ÿè®¡å’ŒæŠ¥å‘Š

ç‰ˆæœ¬ï¼š1.0
"""

import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Tuple
from collections import defaultdict, Counter

class IntegratedExtractor:
    """é›†æˆå®ä½“æŠ½å–å™¨"""
    
    def __init__(self):
        self.model_reliability = {
            'doubao': 3,    # æœ€å¯é 
            'deepseek': 2,  # æ¬¡ä¹‹
            'tongyi': 1     # æœ€å
        }

        self.extractors = {
            'deepseek': 'zhenjiu_extractor_ds.py',
            'doubao': 'zhenjiu_extractor_doubao.py',  # ä½¿ç”¨ç®€åŒ–ç‰ˆ
            'tongyi': 'zhenjiu_extractor_tongyi.py'
        }

        # å…³ç³»æŠ½å–é…ç½®
        self.enable_relation_extraction = True  # æ˜¯å¦å¯ç”¨å…³ç³»æŠ½å–
        self.relation_types = ['TREAT', 'MANIFEST', 'MAIN_TREAT']
        self.relation_extraction_mode = 'after_all_entities'  # 'progressive' æˆ– 'after_all_entities'
        self.processed_docs_count = 0  # å·²å¤„ç†æ–‡æ¡£è®¡æ•°

        # è°ƒè¯•é…ç½®
        self.show_prompts = True  # æ˜¯å¦æ˜¾ç¤ºpromptåˆ°ç»ˆç«¯

        self.review_database = self.load_review_database()
        
    def load_review_database(self) -> Dict[str, Any]:
        """åŠ è½½äººå·¥å®¡æ ¸æ•°æ®åº“"""
        db_file = "review_database.json"
        if os.path.exists(db_file):
            try:
                with open(db_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        
        return {
            'reviews': {},  # entity_text -> review_result
            'statistics': {
                'total_reviewed': 0,
                'confirmed_entities': 0,
                'rejected_entities': 0,
                'type_corrections': 0
            },
            'reviewers': {},
            'last_updated': None
        }
    
    def save_review_database(self):
        """ä¿å­˜äººå·¥å®¡æ ¸æ•°æ®åº“"""
        self.review_database['last_updated'] = time.strftime("%Y-%m-%d %H:%M:%S")
        with open("review_database.json", 'w', encoding='utf-8') as f:
            json.dump(self.review_database, f, ensure_ascii=False, indent=2)
    
    def get_text_files(self, directory: str = "test_texts") -> List[Path]:
        """è·å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶"""
        text_dir = Path(directory)
        if not text_dir.exists():
            print(f"âŒ æ–‡æœ¬ç›®å½•ä¸å­˜åœ¨: {directory}")
            return []
        
        files = list(text_dir.glob("*.txt"))
        print(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
        return files
    
    def call_extractor_directly(self, model_name: str, text_content: str, doc_title: str) -> Dict[str, Any]:
        """ç›´æ¥è°ƒç”¨æŠ½å–å™¨æ¨¡å—"""
        try:
            if model_name == 'deepseek':
                # å¯¼å…¥DeepSeekæŠ½å–å™¨
                sys.path.append('.')
                from zhenjiu_extractor_ds import ZhenjiuEntityExtractor
                extractor = ZhenjiuEntityExtractor()
                return extractor.extract_entities(text_content, doc_title)
                
            elif model_name == 'tongyi':
                # å¯¼å…¥é€šä¹‰åƒé—®æŠ½å–å™¨
                from zhenjiu_extractor_tongyi import ZhenjiuEntityExtractor as TongyiExtractor
                extractor = TongyiExtractor()
                return extractor.extract_entities(text_content, doc_title)
                
            elif model_name == 'doubao':
                # å¯¼å…¥è±†åŒ…æŠ½å–å™¨
                from zhenjiu_extractor_doubao import ZhenjiuEntityExtractor as DoubaoExtractor
                extractor = DoubaoExtractor()
                return extractor.extract_entities(text_content, doc_title)
                
            else:
                return {'success': False, 'error': f'æœªçŸ¥æ¨¡å‹: {model_name}'}
                
        except Exception as e:
            return {'success': False, 'error': f'{model_name} è°ƒç”¨å¤±è´¥: {str(e)}'}
    
    def extract_from_file(self, text_file: Path) -> Dict[str, Any]:
        """ä»å•ä¸ªæ–‡ä»¶æŠ½å–å®ä½“å’Œå…³ç³»ï¼ˆåˆ†é˜¶æ®µï¼‰"""
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {text_file.name}")

        # å¢åŠ å¤„ç†è®¡æ•°
        self.processed_docs_count += 1

        # åˆ¤æ–­æ˜¯å¦å¯ç”¨å…³ç³»æŠ½å–ï¼ˆæ ¹æ®æ¨¡å¼å†³å®šï¼‰
        if self.relation_extraction_mode == 'after_all_entities':
            enable_relations = False  # åœ¨è¿™ä¸ªé˜¶æ®µä¸æŠ½å–å…³ç³»
            print(f"  ğŸ“ ç¬¬ {self.processed_docs_count} ç¯‡æ–‡æ¡£ï¼Œä»…æŠ½å–å®ä½“ï¼ˆå…³ç³»å°†åœ¨æ‰€æœ‰å®ä½“å®Œæˆåç»Ÿä¸€æŠ½å–ï¼‰")
        else:
            # åŸæ¥çš„æ¸è¿›å¼æ¨¡å¼
            enable_relations = (self.enable_relation_extraction and
                              self.processed_docs_count > 50)
            if enable_relations:
                print(f"  ğŸ”— ç¬¬ {self.processed_docs_count} ç¯‡æ–‡æ¡£ï¼Œå¯ç”¨å…³ç³»æŠ½å–")
            else:
                print(f"  ğŸ“ ç¬¬ {self.processed_docs_count} ç¯‡æ–‡æ¡£ï¼Œä»…æŠ½å–å®ä½“")

        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            with open(text_file, 'r', encoding='utf-8') as f:
                text_content = f.read().strip()
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return {}

        doc_title = text_file.stem.replace('_', ' ')
        results = {}

        # ä¾æ¬¡è°ƒç”¨ä¸‰ä¸ªæ¨¡å‹
        for model_name in ['doubao', 'deepseek', 'tongyi']:  # æŒ‰å¯é æ€§æ’åº
            print(f"  ğŸ”„ è¿è¡Œ {model_name} æ¨¡å‹...")

            if enable_relations:
                # è°ƒç”¨ç»¼åˆæŠ½å–æ–¹æ³•ï¼ˆå®ä½“+å…³ç³»ï¼‰
                result = self.call_extractor_with_relations(model_name, text_content, doc_title)
            else:
                # åªè°ƒç”¨å®ä½“æŠ½å–
                result = self.call_extractor_directly(model_name, text_content, doc_title)

            results[model_name] = result

            if result['success']:
                entity_count = len(result.get('entities', []))
                relation_count = len(result.get('relations', []))
                if enable_relations:
                    print(f"    âœ… {model_name}: {entity_count} ä¸ªå®ä½“, {relation_count} ä¸ªå…³ç³»")
                else:
                    print(f"    âœ… {model_name}: {entity_count} ä¸ªå®ä½“")
            else:
                print(f"    âŒ {model_name}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            time.sleep(2)

        return results

    def call_extractor_with_relations(self, model_name: str, text_content: str, doc_title: str) -> Dict[str, Any]:
        """è°ƒç”¨æŠ½å–å™¨è¿›è¡Œå®ä½“å’Œå…³ç³»æŠ½å–"""
        try:
            if model_name == 'deepseek':
                # å¯¼å…¥DeepSeekæŠ½å–å™¨
                sys.path.append('.')
                from zhenjiu_extractor_ds import ZhenjiuEntityExtractor
                extractor = ZhenjiuEntityExtractor()
                return extractor.extract_entities_and_relations(text_content, doc_title)

            elif model_name == 'doubao':
                # å¯¼å…¥è±†åŒ…æŠ½å–å™¨
                sys.path.append('.')
                from zhenjiu_extractor_doubao import ZhenjiuEntityExtractor
                extractor = ZhenjiuEntityExtractor()
                return extractor.extract_entities_and_relations(text_content, doc_title)

            elif model_name == 'tongyi':
                # å¯¼å…¥é€šä¹‰åƒé—®æŠ½å–å™¨
                sys.path.append('.')
                from zhenjiu_extractor_tongyi import ZhenjiuEntityExtractor
                extractor = ZhenjiuEntityExtractor()
                return extractor.extract_entities_and_relations(text_content, doc_title)

            else:
                return {'success': False, 'error': f'æœªçŸ¥æ¨¡å‹: {model_name}'}

        except Exception as e:
            return {'success': False, 'error': f'{model_name} å…³ç³»æŠ½å–å¤±è´¥: {str(e)}'}
    
    def analyze_consistency(self, model_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå®ä½“å’Œå…³ç³»çš„ä¸€è‡´æ€§"""

        # æ”¶é›†æ‰€æœ‰å®ä½“
        all_entities = defaultdict(list)  # (text, type) -> [model_names]
        entity_details = {}

        # æ”¶é›†æ‰€æœ‰å…³ç³»
        all_relations = defaultdict(list)  # (head, relation, tail) -> [model_names]
        relation_details = {}

        for model_name, result in model_results.items():
            if result['success']:
                # å¤„ç†å®ä½“
                for entity in result.get('entities', []):
                    key = (entity['text'], entity['type'])
                    all_entities[key].append(model_name)
                    entity_details[key] = entity

                # å¤„ç†å…³ç³»
                for relation in result.get('relations', []):
                    key = (relation['head'], relation['relation'], relation['tail'])
                    all_relations[key].append(model_name)
                    relation_details[key] = relation
        
        # æŒ‰ä¸€è‡´æ€§åˆ†ç±»
        consensus_data = {
            'unanimous': [],      # ä¸‰æ¨¡å‹ä¸€è‡´
            'majority': [],       # ä¸¤æ¨¡å‹ä¸€è‡´
            'disputed': [],       # ä»…ä¸€ä¸ªæ¨¡å‹
            'conflicts': []       # åŒæ–‡æœ¬ä¸åŒç±»å‹
        }

        # å…³ç³»ä¸€è‡´æ€§åˆ†æ
        relation_consensus = {
            'unanimous': [],      # ä¸‰æ¨¡å‹ä¸€è‡´
            'majority': [],       # ä¸¤æ¨¡å‹ä¸€è‡´
            'disputed': [],       # ä»…ä¸€ä¸ªæ¨¡å‹
            'conflicts': []       # å…³ç³»å†²çª
        }
        
        # æ£€æŸ¥ç±»å‹å†²çª
        text_to_types = defaultdict(dict)
        for (text, entity_type), models in all_entities.items():
            text_to_types[text][entity_type] = models
        
        # è¯†åˆ«å†²çª
        for text, type_models in text_to_types.items():
            if len(type_models) > 1:  # åŒä¸€æ–‡æœ¬æœ‰å¤šç§ç±»å‹
                consensus_data['conflicts'].append({
                    'text': text,
                    'type_assignments': {
                        entity_type: models for entity_type, models in type_models.items()
                    }
                })
        
        # åˆ†ç±»å®ä½“
        for (text, entity_type), models in all_entities.items():
            support_count = len(models)
            
            entity_info = {
                'text': text,
                'type': entity_type,
                'supporting_models': models,
                'support_count': support_count,
                'reliability_score': self.calculate_reliability_score(models),
                'details': entity_details[(text, entity_type)]
            }
            
            if support_count == 3:
                consensus_data['unanimous'].append(entity_info)
            elif support_count == 2:
                consensus_data['majority'].append(entity_info)
            else:
                consensus_data['disputed'].append(entity_info)

        # åˆ†æå…³ç³»ä¸€è‡´æ€§
        for (head, relation, tail), models in all_relations.items():
            support_count = len(models)

            relation_info = {
                'head': head,
                'relation': relation,
                'tail': tail,
                'supporting_models': models,
                'support_count': support_count,
                'reliability_score': self.calculate_reliability_score(models),
                'details': relation_details[(head, relation, tail)]
            }

            if support_count == 3:
                relation_consensus['unanimous'].append(relation_info)
            elif support_count == 2:
                relation_consensus['majority'].append(relation_info)
            else:
                relation_consensus['disputed'].append(relation_info)

        return {
            'entities': consensus_data,
            'relations': relation_consensus,
            'has_relations': len(all_relations) > 0
        }
    
    def calculate_reliability_score(self, models: List[str]) -> float:
        """è®¡ç®—å¯é æ€§åˆ†æ•°"""
        total_score = sum(self.model_reliability.get(model, 0) for model in models)
        max_possible = len(models) * 3  # æœ€é«˜å¯é æ€§æ˜¯3
        return total_score / max_possible if max_possible > 0 else 0
    
    def identify_review_candidates(self, consensus_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«éœ€è¦äººå·¥å®¡æ ¸çš„å€™é€‰å®ä½“"""
        
        candidates = []
        
        # 1. ç±»å‹å†²çªï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        for conflict in consensus_data['conflicts']:
            text = conflict['text']
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å®¡æ ¸è¿‡
            if text in self.review_database['reviews']:
                continue
            
            candidates.append({
                'text': text,
                'priority': 'high',
                'reason': 'type_conflict',
                'conflict_info': conflict,
                'question': f'å®ä½“"{text}"çš„æ­£ç¡®ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ'
            })
        
        # 2. ä»…è¢«ä¸€ä¸ªæ¨¡å‹è¯†åˆ«çš„å®ä½“ï¼ˆæŒ‰å¯é æ€§æ’åºï¼‰
        disputed_by_reliability = sorted(
            consensus_data['disputed'],
            key=lambda x: self.model_reliability.get(x['supporting_models'][0], 0),
            reverse=True
        )
        
        for entity in disputed_by_reliability[:10]:  # é™åˆ¶æ•°é‡
            text = entity['text']
            
            if text in self.review_database['reviews']:
                continue
            
            model = entity['supporting_models'][0]
            candidates.append({
                'text': text,
                'priority': 'medium',
                'reason': 'single_model',
                'entity_info': entity,
                'question': f'å®ä½“"{text}"ï¼ˆä»…è¢«{model}è¯†åˆ«ï¼‰æ˜¯å¦æœ‰æ•ˆï¼Ÿ'
            })
        
        return candidates
    
    def save_review_tasks(self, candidates: List[Dict[str, Any]],
                         doc_name: str, doc_content: str) -> str:
        """ä¿å­˜å®¡æ ¸ä»»åŠ¡åˆ°outputs/review_data/human_review_tasks.json"""

        if not candidates:
            print("âœ… æ— éœ€äººå·¥å®¡æ ¸")
            return "outputs/review_data/human_review_tasks.json"

        # åŠ è½½ç°æœ‰çš„å®¡æ ¸ä»»åŠ¡
        review_tasks_file = "outputs/review_data/human_review_tasks.json"
        existing_tasks = []

        if os.path.exists(review_tasks_file):
            try:
                with open(review_tasks_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_tasks = existing_data.get('tasks', [])
            except:
                existing_tasks = []

        # ç”Ÿæˆæ–°çš„å®¡æ ¸ä»»åŠ¡
        new_tasks = []
        task_id_start = len(existing_tasks) + 1

        for i, candidate in enumerate(candidates):
            text = candidate['text']
            context = self.extract_context(doc_content, text)

            # æ„å»ºæ¨¡å‹æ„è§ - æ˜¾ç¤ºæ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹çš„åˆ¤æ–­
            models_opinion = {}

            # å…ˆåˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ä¸º"æœªè¯†åˆ«"
            for model_name in ['doubao', 'deepseek', 'tongyi']:
                models_opinion[model_name] = 'æœªè¯†åˆ«'

            if candidate['reason'] == 'type_conflict':
                conflict_info = candidate['conflict_info']
                for entity_type, models in conflict_info['type_assignments'].items():
                    for model in models:
                        models_opinion[model] = entity_type
            elif candidate['reason'] == 'single_model':
                entity_info = candidate['entity_info']
                model = entity_info['supporting_models'][0]
                entity_type = entity_info['type']
                models_opinion[model] = entity_type

            task = {
                'id': task_id_start + i,
                'doc_name': doc_name,
                'entity_text': text,
                'priority': candidate['priority'],
                'reason': candidate['reason'],
                'question': candidate['question'],
                'context': {
                    'highlighted': context,
                    'full_sentence': self.extract_sentence(doc_content, text)
                },
                'models_opinion': models_opinion,
                'options': ['ç—…å', 'ç—‡çŠ¶', 'ç©´ä½', 'æ²»æ³•', 'æ— æ•ˆ'],
                'human_decision': {
                    'selected_option': '',  # å¾…å¡«å†™
                    'notes': '',           # å¤‡æ³¨
                    'reviewer': '',        # å®¡æ ¸äºº
                    'timestamp': ''        # å®¡æ ¸æ—¶é—´
                }
            }
            new_tasks.append(task)

        # åˆå¹¶ä»»åŠ¡
        all_tasks = existing_tasks + new_tasks

        # ä¿å­˜å®¡æ ¸ä»»åŠ¡
        review_data = {
            'instructions': """
ğŸ” é’ˆç¸å¤§æˆå®ä½“æŠ½å– - äººå·¥å®¡æ ¸ä»»åŠ¡

è¯·å¯¹ä»¥ä¸‹äº‰è®®å®ä½“è¿›è¡Œå®¡æ ¸ï¼š

ğŸ“‹ å®¡æ ¸è¯´æ˜ï¼š
1. ä»”ç»†é˜…è¯»ä¸Šä¸‹æ–‡ï¼Œç†è§£å®ä½“åœ¨åŸæ–‡ä¸­çš„å«ä¹‰
2. å‚è€ƒå„æ¨¡å‹çš„åˆ¤æ–­ï¼Œä½†ä»¥æ‚¨çš„ä¸“ä¸šçŸ¥è¯†ä¸ºå‡†
3. ä¼˜å…ˆå¤„ç†"é«˜ä¼˜å…ˆçº§"ä»»åŠ¡ï¼ˆç±»å‹å†²çªï¼‰

âœ… å®¡æ ¸é€‰é¡¹ï¼š
- ç—…åï¼šç–¾ç—…åç§°ï¼ˆå¦‚ï¼šè‚çƒ­ç—…ã€å¿ƒçƒ­ç—…ï¼‰
- ç—‡çŠ¶ï¼šç—‡çŠ¶è¡¨ç°ï¼ˆå¦‚ï¼šè…¹ç—›ã€å¤´ç—›ã€å°ä¾¿å…ˆé»„ï¼‰
- ç©´ä½ï¼šç»ç»œç©´ä½ï¼ˆå¦‚ï¼šè¶³å¥é˜´ã€æ‰‹å°‘é˜´ã€ç™¾ä¼šï¼‰
- æ²»æ³•ï¼šæ²»ç–—æ–¹æ³•ï¼ˆå¦‚ï¼šåˆºã€ç¸ã€è¡¥ã€æ³»ï¼‰
- æ— æ•ˆï¼šä¸æ˜¯æœ‰æ•ˆå®ä½“
            """,
            'total_tasks': len(all_tasks),
            'new_tasks_count': len(new_tasks),
            'tasks': all_tasks
        }

        with open(review_tasks_file, 'w', encoding='utf-8') as f:
            json.dump(review_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“ å·²æ·»åŠ  {len(new_tasks)} ä¸ªå®¡æ ¸ä»»åŠ¡åˆ° {review_tasks_file}")
        print(f"ğŸ“Š æ€»å®¡æ ¸ä»»åŠ¡æ•°: {len(all_tasks)}")

        return review_tasks_file

    def extract_sentence(self, text: str, entity_text: str) -> str:
        """æå–åŒ…å«å®ä½“çš„å®Œæ•´å¥å­"""
        pos = text.find(entity_text)
        if pos == -1:
            return entity_text

        # ä¸­æ–‡å¥å­åˆ†éš”ç¬¦
        sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', '\n']

        # å‘å‰æ‰¾å¥å­å¼€å§‹
        start = pos
        while start > 0 and text[start-1] not in sentence_endings:
            start -= 1

        # å‘åæ‰¾å¥å­ç»“æŸ
        end = pos + len(entity_text)
        while end < len(text) and text[end] not in sentence_endings:
            end += 1

        if end < len(text):
            end += 1  # åŒ…å«å¥å·

        sentence = text[start:end].strip()
        return sentence
    
    def extract_context(self, text: str, entity_text: str, context_length: int = 80) -> str:
        """æå–å®ä½“ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬ä¸Šä¸€å¥å’Œä¸‹ä¸€å¥ï¼‰"""
        pos = text.find(entity_text)
        if pos == -1:
            return "æœªæ‰¾åˆ°ä¸Šä¸‹æ–‡"

        # æ‰¾åˆ°å®ä½“æ‰€åœ¨å¥å­çš„è¾¹ç•Œ
        entity_start = pos
        entity_end = pos + len(entity_text)

        # å‘å‰æ‰¾å¥å­è¾¹ç•Œï¼ˆæ‰¾åˆ°å¥å·ã€æ„Ÿå¹å·ã€é—®å·æˆ–æ–‡æ¡£å¼€å¤´ï¼‰
        sentence_start = entity_start
        while sentence_start > 0:
            if text[sentence_start - 1] in 'ã€‚ï¼ï¼Ÿ\n':
                break
            sentence_start -= 1

        # å‘åæ‰¾å¥å­è¾¹ç•Œ
        sentence_end = entity_end
        while sentence_end < len(text):
            if text[sentence_end] in 'ã€‚ï¼ï¼Ÿ\n':
                sentence_end += 1  # åŒ…å«æ ‡ç‚¹
                break
            sentence_end += 1

        # æ‰©å±•åˆ°ä¸Šä¸€å¥
        prev_sentence_start = sentence_start
        if sentence_start > 0:
            # å‘å‰æ‰¾ä¸Šä¸€å¥çš„å¼€å§‹
            temp_pos = sentence_start - 1
            while temp_pos > 0 and text[temp_pos] in 'ã€‚ï¼ï¼Ÿ\n ':
                temp_pos -= 1  # è·³è¿‡æ ‡ç‚¹å’Œç©ºæ ¼

            # æ‰¾åˆ°ä¸Šä¸€å¥çš„å¼€å§‹
            while temp_pos > 0:
                if text[temp_pos - 1] in 'ã€‚ï¼ï¼Ÿ\n':
                    break
                temp_pos -= 1
            prev_sentence_start = temp_pos

        # æ‰©å±•åˆ°ä¸‹ä¸€å¥
        next_sentence_end = sentence_end
        if sentence_end < len(text):
            # å‘åæ‰¾ä¸‹ä¸€å¥çš„ç»“æŸ
            temp_pos = sentence_end
            while temp_pos < len(text) and text[temp_pos] in ' \n':
                temp_pos += 1  # è·³è¿‡ç©ºæ ¼å’Œæ¢è¡Œ

            # æ‰¾åˆ°ä¸‹ä¸€å¥çš„ç»“æŸ
            while temp_pos < len(text):
                if text[temp_pos] in 'ã€‚ï¼ï¼Ÿ\n':
                    temp_pos += 1  # åŒ…å«æ ‡ç‚¹
                    break
                temp_pos += 1
            next_sentence_end = temp_pos

        # æå–æ‰©å±•çš„ä¸Šä¸‹æ–‡
        extended_context = text[prev_sentence_start:next_sentence_end].strip()

        # é«˜äº®å®ä½“
        highlighted = extended_context.replace(entity_text, f"ã€{entity_text}ã€‘")

        return highlighted

    def generate_final_entities(self, consensus_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæœ€ç»ˆå®ä½“åˆ—è¡¨ï¼ˆæŒ‰å¯é æ€§æ’åºï¼‰"""

        final_entities = []

        # 1. ä¸‰æ¨¡å‹ä¸€è‡´çš„å®ä½“ï¼ˆç›´æ¥é‡‡ç”¨ï¼Œæœ€é«˜ç½®ä¿¡åº¦ï¼‰
        for entity in consensus_data['unanimous']:
            final_entities.append({
                'text': entity['text'],
                'type': entity['type'],
                'confidence': 'high',
                'source': 'unanimous',
                'supporting_models': entity['supporting_models'],
                'reliability_score': entity['reliability_score']
            })

        # 2. ä¸¤æ¨¡å‹ä¸€è‡´çš„å®ä½“ï¼ˆæŒ‰å¯é æ€§é‡‡ç”¨ï¼‰
        for entity in consensus_data['majority']:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è±†åŒ…ï¼ˆæœ€å¯é ï¼‰
            models = entity['supporting_models']
            if 'doubao' in models:
                confidence = 'high'
            elif 'deepseek' in models:
                confidence = 'medium'
            else:
                confidence = 'low'

            final_entities.append({
                'text': entity['text'],
                'type': entity['type'],
                'confidence': confidence,
                'source': 'majority',
                'supporting_models': entity['supporting_models'],
                'reliability_score': entity['reliability_score']
            })

        # 3. å•æ¨¡å‹å®ä½“ï¼ˆä»…é‡‡ç”¨è±†åŒ…çš„ï¼Œæˆ–å·²å®¡æ ¸çš„ï¼‰
        for entity in consensus_data['disputed']:
            model = entity['supporting_models'][0]
            text = entity['text']

            # æ£€æŸ¥äººå·¥å®¡æ ¸ç»“æœ
            if text in self.review_database['reviews']:
                review = self.review_database['reviews'][text]
                if review['selected_type'] != 'æ— æ•ˆ':
                    final_entities.append({
                        'text': text,
                        'type': review['selected_type'],
                        'confidence': review['confidence'].lower(),
                        'source': 'human_reviewed',
                        'supporting_models': ['human_review'],
                        'reliability_score': 1.0
                    })
            elif model == 'doubao':  # ä»…é‡‡ç”¨è±†åŒ…çš„å•æ¨¡å‹å®ä½“
                final_entities.append({
                    'text': entity['text'],
                    'type': entity['type'],
                    'confidence': 'medium',
                    'source': 'doubao_only',
                    'supporting_models': entity['supporting_models'],
                    'reliability_score': entity['reliability_score']
                })

        # 4. å¤„ç†ç±»å‹å†²çªï¼ˆä½¿ç”¨äººå·¥å®¡æ ¸ç»“æœæˆ–è±†åŒ…åˆ¤æ–­ï¼‰
        for conflict in consensus_data['conflicts']:
            text = conflict['text']

            if text in self.review_database['reviews']:
                review = self.review_database['reviews'][text]
                if review['selected_type'] != 'æ— æ•ˆ':
                    final_entities.append({
                        'text': text,
                        'type': review['selected_type'],
                        'confidence': review['confidence'].lower(),
                        'source': 'conflict_resolved',
                        'supporting_models': ['human_review'],
                        'reliability_score': 1.0
                    })
            else:
                # ä½¿ç”¨è±†åŒ…çš„åˆ¤æ–­ï¼Œå¦‚æœè±†åŒ…æ²¡æœ‰åˆ™ç”¨DeepSeek
                best_type = None
                best_models = []

                for entity_type, models in conflict['type_assignments'].items():
                    if 'doubao' in models:
                        best_type = entity_type
                        best_models = models
                        break
                    elif 'deepseek' in models and not best_type:
                        best_type = entity_type
                        best_models = models

                if best_type:
                    final_entities.append({
                        'text': text,
                        'type': best_type,
                        'confidence': 'low',
                        'source': 'conflict_auto_resolved',
                        'supporting_models': best_models,
                        'reliability_score': self.calculate_reliability_score(best_models)
                    })

        return final_entities

    def generate_final_relations(self, relation_consensus: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæœ€ç»ˆå…³ç³»åˆ—è¡¨ï¼ˆæŒ‰å¯é æ€§æ’åºï¼‰"""

        final_relations = []

        # 1. ä¸‰æ¨¡å‹ä¸€è‡´çš„å…³ç³»ï¼ˆç›´æ¥é‡‡ç”¨ï¼Œæœ€é«˜ç½®ä¿¡åº¦ï¼‰
        for relation in relation_consensus.get('unanimous', []):
            final_relations.append({
                'head': relation['head'],
                'relation': relation['relation'],
                'tail': relation['tail'],
                'confidence': 'high',
                'source': 'unanimous_consensus',
                'supporting_models': relation['supporting_models'],
                'reliability_score': relation['reliability_score']
            })

        # 2. ä¸¤æ¨¡å‹ä¸€è‡´çš„å…³ç³»ï¼ˆé‡‡ç”¨ï¼Œä¸­ç­‰ç½®ä¿¡åº¦ï¼‰
        for relation in relation_consensus.get('majority', []):
            final_relations.append({
                'head': relation['head'],
                'relation': relation['relation'],
                'tail': relation['tail'],
                'confidence': 'medium',
                'source': 'majority_consensus',
                'supporting_models': relation['supporting_models'],
                'reliability_score': relation['reliability_score']
            })

        # 3. å•æ¨¡å‹å…³ç³»ï¼ˆä»…é‡‡ç”¨è±†åŒ…çš„ï¼‰
        for relation in relation_consensus.get('disputed', []):
            model = relation['supporting_models'][0]

            # åªé‡‡ç”¨è±†åŒ…çš„å•æ¨¡å‹å…³ç³»
            if model == 'doubao':
                final_relations.append({
                    'head': relation['head'],
                    'relation': relation['relation'],
                    'tail': relation['tail'],
                    'confidence': 'low',
                    'source': 'doubao_only',
                    'supporting_models': relation['supporting_models'],
                    'reliability_score': relation['reliability_score']
                })

        return final_relations

    def extract_relations_for_all_documents(self, entity_results_dir: str = "outputs/integrated_results") -> Dict[str, Any]:
        """ä¸ºæ‰€æœ‰å·²å®Œæˆçš„å®ä½“æŠ½å–æ–‡æ¡£ç»Ÿä¸€æŠ½å–å…³ç³»"""

        print("\n" + "="*80)
        print("ğŸ”— å¼€å§‹ä¸ºæ‰€æœ‰æ–‡æ¡£ç»Ÿä¸€æŠ½å–å…³ç³»")
        print("="*80)

        # æŸ¥æ‰¾æ‰€æœ‰å®ä½“ç»“æœæ–‡ä»¶
        import glob
        entity_files = glob.glob(f"{entity_results_dir}/final_entities_*.json")

        if not entity_files:
            print("âŒ æœªæ‰¾åˆ°å®ä½“ç»“æœæ–‡ä»¶")
            return {'success': False, 'error': 'æœªæ‰¾åˆ°å®ä½“ç»“æœæ–‡ä»¶'}

        print(f"ğŸ“ æ‰¾åˆ° {len(entity_files)} ä¸ªå®ä½“ç»“æœæ–‡ä»¶")

        relation_results = {}
        total_relations = 0

        for entity_file in entity_files:
            try:
                # è¯»å–å®ä½“ç»“æœ
                with open(entity_file, 'r', encoding='utf-8') as f:
                    entity_data = json.load(f)

                doc_name = entity_data.get('doc_name', 'unknown')
                entities = entity_data.get('entities', [])

                if not entities:
                    print(f"âš ï¸ {doc_name}: æ— å®ä½“æ•°æ®ï¼Œè·³è¿‡")
                    continue

                print(f"\nğŸ”„ å¤„ç†æ–‡æ¡£: {doc_name}")
                print(f"   å®ä½“æ•°é‡: {len(entities)}")

                # è¯»å–åŸæ–‡æ¡£å†…å®¹
                doc_content = self.load_document_content(doc_name)
                if not doc_content:
                    print(f"âš ï¸ {doc_name}: æ— æ³•è¯»å–åŸæ–‡æ¡£ï¼Œè·³è¿‡")
                    continue

                # ä¸ºè¿™ä¸ªæ–‡æ¡£æŠ½å–å…³ç³»
                doc_relations = self.extract_relations_for_document(doc_name, doc_content, entities)

                if doc_relations['success']:
                    relation_count = len(doc_relations['relations'])
                    total_relations += relation_count
                    print(f"   âœ… æŠ½å–åˆ° {relation_count} ä¸ªå…³ç³»")

                    # ä¿å­˜å…³ç³»ç»“æœ
                    self.save_relation_results(doc_name, doc_relations['relations'], entities)

                    relation_results[doc_name] = doc_relations
                else:
                    print(f"   âŒ å…³ç³»æŠ½å–å¤±è´¥: {doc_relations.get('error', 'æœªçŸ¥é”™è¯¯')}")

            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {entity_file} å¤±è´¥: {e}")

        print(f"\nğŸ‰ å…³ç³»æŠ½å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡æŠ½å– {total_relations} ä¸ªå…³ç³»")

        return {
            'success': True,
            'total_documents': len(entity_files),
            'total_relations': total_relations,
            'results': relation_results
        }

    def load_document_content(self, doc_name: str) -> str:
        """åŠ è½½æ–‡æ¡£åŸå§‹å†…å®¹"""
        # å°è¯•ä¸åŒçš„æ–‡ä»¶åæ ¼å¼
        possible_files = [
            f"test_texts/{doc_name}.txt",
            f"test_texts/{doc_name.replace(' ', '_')}.txt",
            f"test_texts/{doc_name.replace(' ', '')}.txt"
        ]

        for file_path in possible_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read().strip()
            except FileNotFoundError:
                continue

        return ""

    def extract_relations_for_document(self, doc_name: str, doc_content: str, entities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä¸ºå•ä¸ªæ–‡æ¡£æŠ½å–å…³ç³»"""

        # è½¬æ¢å®ä½“æ ¼å¼
        formatted_entities = []
        for entity in entities:
            formatted_entities.append({
                'text': entity['text'],
                'type': entity['type']
            })

        # è°ƒç”¨ä¸‰ä¸ªæ¨¡å‹æŠ½å–å…³ç³»
        model_results = {}

        for model_name in ['doubao', 'deepseek', 'tongyi']:
            try:
                print(f"     ğŸ”„ {model_name} æ¨¡å‹æŠ½å–å…³ç³»...")

                result = self.call_model_for_relations(model_name, doc_content, formatted_entities, doc_name)
                model_results[model_name] = result

                if result['success']:
                    relation_count = len(result.get('relations', []))
                    print(f"       âœ… {relation_count} ä¸ªå…³ç³»")
                else:
                    print(f"       âŒ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                # æ·»åŠ å»¶è¿Ÿ
                time.sleep(1)

            except Exception as e:
                print(f"       âŒ {model_name} æŠ½å–å¤±è´¥: {e}")
                model_results[model_name] = {'success': False, 'error': str(e)}

        # åˆ†æå…³ç³»ä¸€è‡´æ€§
        final_relations = self.analyze_relation_consensus(model_results)

        return {
            'success': True,
            'relations': final_relations,
            'model_results': model_results
        }

    def call_model_for_relations(self, model_name: str, text_content: str, entities: List[Dict[str, Any]], doc_title: str) -> Dict[str, Any]:
        """è°ƒç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œå…³ç³»æŠ½å–"""
        try:
            if model_name == 'deepseek':
                from zhenjiu_extractor_ds import ZhenjiuEntityExtractor
                extractor = ZhenjiuEntityExtractor()
                return extractor.extract_relations(text_content, entities, doc_title)

            elif model_name == 'doubao':
                from zhenjiu_extractor_doubao import ZhenjiuEntityExtractor
                extractor = ZhenjiuEntityExtractor()
                return extractor.extract_relations(text_content, entities, doc_title)

            elif model_name == 'tongyi':
                from zhenjiu_extractor_tongyi import ZhenjiuEntityExtractor
                extractor = ZhenjiuEntityExtractor()
                return extractor.extract_relations(text_content, entities, doc_title)

            else:
                return {'success': False, 'error': f'æœªçŸ¥æ¨¡å‹: {model_name}'}

        except Exception as e:
            return {'success': False, 'error': f'{model_name} å…³ç³»æŠ½å–å¤±è´¥: {str(e)}'}

    def analyze_relation_consensus(self, model_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†æå…³ç³»ä¸€è‡´æ€§å¹¶ç”Ÿæˆæœ€ç»ˆå…³ç³»åˆ—è¡¨"""

        all_relations = defaultdict(list)  # (head, relation, tail) -> [model_names]
        relation_details = {}

        # æ”¶é›†æ‰€æœ‰å…³ç³»
        for model_name, result in model_results.items():
            if result.get('success'):
                for relation in result.get('relations', []):
                    key = (relation['head'], relation['relation'], relation['tail'])
                    all_relations[key].append(model_name)
                    relation_details[key] = relation

        final_relations = []

        # æŒ‰ä¸€è‡´æ€§ç”Ÿæˆæœ€ç»ˆå…³ç³»
        for (head, relation, tail), models in all_relations.items():
            support_count = len(models)

            if support_count >= 2:  # è‡³å°‘ä¸¤ä¸ªæ¨¡å‹æ”¯æŒ
                confidence = 'high' if support_count == 3 else 'medium'
                final_relations.append({
                    'head': head,
                    'relation': relation,
                    'tail': tail,
                    'confidence': confidence,
                    'supporting_models': models,
                    'support_count': support_count
                })
            elif 'doubao' in models:  # åªæœ‰è±†åŒ…æ”¯æŒçš„å…³ç³»ä¹Ÿé‡‡ç”¨
                final_relations.append({
                    'head': head,
                    'relation': relation,
                    'tail': tail,
                    'confidence': 'low',
                    'supporting_models': models,
                    'support_count': support_count
                })

        return final_relations

    def save_relation_results(self, doc_name: str, relations: List[Dict[str, Any]], entities: List[Dict[str, Any]]):
        """ä¿å­˜å…³ç³»æŠ½å–ç»“æœ"""

        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜å…³ç³»ç»“æœ
        relation_result = {
            'doc_name': doc_name,
            'timestamp': timestamp,
            'entities': entities,
            'relations': relations,
            'statistics': {
                'entity_count': len(entities),
                'relation_count': len(relations),
                'relation_types': {
                    rel_type: len([r for r in relations if r['relation'] == rel_type])
                    for rel_type in self.relation_types
                }
            }
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        relation_file = f"outputs/integrated_results/relations_{doc_name}_{timestamp}.json"
        with open(relation_file, 'w', encoding='utf-8') as f:
            json.dump(relation_result, f, ensure_ascii=False, indent=2)

        print(f"       ğŸ’¾ å…³ç³»ç»“æœå·²ä¿å­˜: {relation_file}")

        # æ›´æ–°åŸæœ‰çš„å®ä½“æ–‡ä»¶ï¼Œæ·»åŠ å…³ç³»ä¿¡æ¯
        import glob
        entity_files = glob.glob(f"outputs/integrated_results/final_entities_{doc_name}_*.json")
        if entity_files:
            latest_entity_file = max(entity_files)
            try:
                with open(latest_entity_file, 'r', encoding='utf-8') as f:
                    entity_data = json.load(f)

                entity_data['relations'] = relations
                entity_data['relation_statistics'] = relation_result['statistics']

                with open(latest_entity_file, 'w', encoding='utf-8') as f:
                    json.dump(entity_data, f, ensure_ascii=False, indent=2)

                print(f"       ğŸ”„ å·²æ›´æ–°å®ä½“æ–‡ä»¶: {latest_entity_file}")

            except Exception as e:
                print(f"       âš ï¸ æ›´æ–°å®ä½“æ–‡ä»¶å¤±è´¥: {e}")

    def generate_bert_training_data(self, text_file: Path, final_entities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”ŸæˆBERT+BiLSTM+CRFè®­ç»ƒæ•°æ®æ ¼å¼"""

        # è¯»å–åŸæ–‡
        with open(text_file, 'r', encoding='utf-8') as f:
            text = f.read().strip()

        # åˆ›å»ºå­—ç¬¦çº§åˆ«çš„æ ‡æ³¨
        char_labels = ['O'] * len(text)

        # å®ä½“ç±»å‹æ˜ å°„
        type_mapping = {
            'ç—…å': 'DIS',    # Disease
            'ç—‡çŠ¶': 'SYM',    # Symptom
            'ç©´ä½': 'ACU',    # Acupoint
            'æ²»æ³•': 'TRE'     # Treatment
        }

        # æ ‡æ³¨å®ä½“
        for entity in final_entities:
            entity_text = entity['text']
            entity_type = entity['type']

            if entity_type not in type_mapping:
                continue

            bio_tag = type_mapping[entity_type]

            # æŸ¥æ‰¾å®ä½“åœ¨æ–‡æœ¬ä¸­çš„æ‰€æœ‰ä½ç½®
            start_pos = 0
            while True:
                pos = text.find(entity_text, start_pos)
                if pos == -1:
                    break

                # æ£€æŸ¥æ˜¯å¦å·²ç»è¢«æ ‡æ³¨
                if char_labels[pos] == 'O':
                    # BIOæ ‡æ³¨
                    for i, char in enumerate(entity_text):
                        if i == 0:
                            char_labels[pos + i] = f'B-{bio_tag}'
                        else:
                            char_labels[pos + i] = f'I-{bio_tag}'

                start_pos = pos + 1

        # ç”Ÿæˆè®­ç»ƒæ•°æ®æ ¼å¼
        training_data = {
            'doc_name': text_file.stem,
            'text': text,
            'entities': final_entities,
            'char_labels': char_labels,
            'bio_format': self.convert_to_bio_format(text, char_labels),
            'statistics': {
                'total_chars': len(text),
                'total_entities': len(final_entities),
                'entity_types': {
                    'ç—…å': len([e for e in final_entities if e['type'] == 'ç—…å']),
                    'ç—‡çŠ¶': len([e for e in final_entities if e['type'] == 'ç—‡çŠ¶']),
                    'ç©´ä½': len([e for e in final_entities if e['type'] == 'ç©´ä½']),
                    'æ²»æ³•': len([e for e in final_entities if e['type'] == 'æ²»æ³•'])
                }
            }
        }

        return training_data

    def convert_to_bio_format(self, text: str, char_labels: List[str]) -> List[Dict[str, str]]:
        """è½¬æ¢ä¸ºBIOæ ¼å¼çš„è®­ç»ƒæ•°æ®"""

        bio_data = []
        for i, char in enumerate(text):
            if char.strip():  # è·³è¿‡ç©ºç™½å­—ç¬¦
                bio_data.append({
                    'char': char,
                    'label': char_labels[i],
                    'position': i
                })

        return bio_data

    def display_results(self, doc_name: str, consensus_data: Dict[str, Any],
                       final_entities: List[Dict[str, Any]], review_stats: Dict[str, Any],
                       final_relations: List[Dict[str, Any]] = None):
        """æ˜¾ç¤ºç»“æœç»Ÿè®¡"""

        print(f"\n{'='*80}")
        print(f"ğŸ“Š {doc_name} - æŠ½å–ç»“æœç»Ÿè®¡")
        print(f"{'='*80}")

        # ä¸€è‡´æ€§ç»Ÿè®¡
        print(f"\nğŸ¤ ä¸€è‡´æ€§åˆ†æ:")
        print(f"   âœ… ä¸‰æ¨¡å‹ä¸€è‡´: {len(consensus_data['unanimous'])} ä¸ª")
        print(f"   ğŸ¤ ä¸¤æ¨¡å‹ä¸€è‡´: {len(consensus_data['majority'])} ä¸ª")
        print(f"   â“ ä»…ä¸€ä¸ªæ¨¡å‹: {len(consensus_data['disputed'])} ä¸ª")
        print(f"   âš ï¸ ç±»å‹å†²çª: {len(consensus_data['conflicts'])} ä¸ª")

        # äººå·¥å®¡æ ¸ç»Ÿè®¡
        if review_stats['reviewed'] > 0 or review_stats['skipped'] > 0:
            print(f"\nğŸ“ äººå·¥å®¡æ ¸:")
            print(f"   âœ… å·²å®¡æ ¸: {review_stats['reviewed']} ä¸ª")
            print(f"   â­ï¸ å·²è·³è¿‡: {review_stats['skipped']} ä¸ª")

        # æœ€ç»ˆå®ä½“ç»Ÿè®¡
        print(f"\nğŸ¯ æœ€ç»ˆå®ä½“: {len(final_entities)} ä¸ª")

        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_stats = defaultdict(int)
        confidence_stats = defaultdict(int)
        source_stats = defaultdict(int)

        for entity in final_entities:
            type_stats[entity['type']] += 1
            confidence_stats[entity['confidence']] += 1
            source_stats[entity['source']] += 1

        print(f"\nğŸ“‹ ç±»å‹åˆ†å¸ƒ:")
        for entity_type in ['ç—…å', 'ç—‡çŠ¶', 'ç©´ä½', 'æ²»æ³•']:
            count = type_stats.get(entity_type, 0)
            print(f"   {entity_type}: {count} ä¸ª")

        print(f"\nğŸ¯ ç½®ä¿¡åº¦åˆ†å¸ƒ:")
        for confidence in ['high', 'medium', 'low']:
            count = confidence_stats.get(confidence, 0)
            percentage = count / len(final_entities) * 100 if final_entities else 0
            print(f"   {confidence}: {count} ä¸ª ({percentage:.1f}%)")

        print(f"\nğŸ“Š æ¥æºåˆ†å¸ƒ:")
        for source, count in source_stats.items():
            percentage = count / len(final_entities) * 100 if final_entities else 0
            print(f"   {source}: {count} ä¸ª ({percentage:.1f}%)")

        # æ˜¾ç¤ºå…³ç³»ç»Ÿè®¡
        if final_relations:
            print(f"\nğŸ”— å…³ç³»æŠ½å–ç»“æœ:")
            print(f"   æ€»å…³ç³»æ•°: {len(final_relations)}")

            # å…³ç³»ç±»å‹åˆ†å¸ƒ
            relation_type_stats = defaultdict(int)
            relation_confidence_stats = defaultdict(int)

            for relation in final_relations:
                relation_type_stats[relation['relation']] += 1
                relation_confidence_stats[relation['confidence']] += 1

            print(f"\nğŸ”— å…³ç³»ç±»å‹åˆ†å¸ƒ:")
            for rel_type, count in relation_type_stats.items():
                percentage = count / len(final_relations) * 100
                print(f"   {rel_type}: {count} ä¸ª ({percentage:.1f}%)")

            print(f"\nğŸ¯ å…³ç³»ç½®ä¿¡åº¦åˆ†å¸ƒ:")
            for confidence in ['high', 'medium', 'low']:
                count = relation_confidence_stats.get(confidence, 0)
                percentage = count / len(final_relations) * 100 if final_relations else 0
                print(f"   {confidence}: {count} ä¸ª ({percentage:.1f}%)")
        else:
            print(f"\nğŸ”— å…³ç³»æŠ½å–: æœªå¯ç”¨ï¼ˆå½“å‰ä¸ºå®ä½“æŠ½å–é˜¶æ®µï¼‰")

    def save_results(self, doc_name: str, model_results: Dict[str, Any],
                    consensus_data: Dict[str, Any], final_entities: List[Dict[str, Any]],
                    review_stats: Dict[str, Any], bert_training_data: Dict[str, Any],
                    final_relations: List[Dict[str, Any]] = None):
        """ä¿å­˜ç»“æœ"""

        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_result = {
            'doc_name': doc_name,
            'timestamp': timestamp,
            'model_results': model_results,
            'consensus_analysis': consensus_data,
            'final_entities': final_entities,
            'final_relations': final_relations or [],
            'review_statistics': review_stats,
            'model_reliability': self.model_reliability,
            'summary': {
                'total_final_entities': len(final_entities),
                'total_final_relations': len(final_relations) if final_relations else 0,
                'unanimous_entities': len(consensus_data['unanimous']),
                'majority_entities': len(consensus_data['majority']),
                'disputed_entities': len(consensus_data['disputed']),
                'conflicts': len(consensus_data['conflicts']),
                'human_reviewed': review_stats['reviewed'],
                'has_relations': bool(final_relations)
            }
        }

        result_file = f"outputs/integrated_results/integrated_result_{doc_name}_{timestamp}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_result, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {result_file}")

        # ä¿å­˜ç®€åŒ–çš„æœ€ç»ˆå®ä½“å’Œå…³ç³»åˆ—è¡¨
        simple_result = {
            'doc_name': doc_name,
            'timestamp': timestamp,
            'entities': [
                {
                    'text': entity['text'],
                    'type': entity['type'],
                    'confidence': entity['confidence']
                }
                for entity in final_entities
            ],
            'relations': [
                {
                    'head': relation['head'],
                    'relation': relation['relation'],
                    'tail': relation['tail'],
                    'confidence': relation['confidence']
                }
                for relation in (final_relations or [])
            ],
            'statistics': detailed_result['summary']
        }

        simple_file = f"outputs/integrated_results/final_entities_{doc_name}_{timestamp}.json"
        with open(simple_file, 'w', encoding='utf-8') as f:
            json.dump(simple_result, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ æœ€ç»ˆå®ä½“å·²ä¿å­˜: {simple_file}")

        # ä¿å­˜BERTè®­ç»ƒæ•°æ®
        bert_file = f"outputs/training_data/bert_training_{doc_name}_{timestamp}.json"
        with open(bert_file, 'w', encoding='utf-8') as f:
            json.dump(bert_training_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ¤– BERTè®­ç»ƒæ•°æ®å·²ä¿å­˜: {bert_file}")

        return result_file, simple_file, bert_file

    def process_single_file(self, text_file: Path) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶çš„å®Œæ•´æµç¨‹"""

        doc_name = text_file.stem.replace('_', ' ')

        # 1. ä¸‰æ¨¡å‹æŠ½å–
        model_results = self.extract_from_file(text_file)

        if not any(result.get('success', False) for result in model_results.values()):
            print(f"âŒ æ‰€æœ‰æ¨¡å‹æŠ½å–å¤±è´¥")
            return {'success': False, 'error': 'æ‰€æœ‰æ¨¡å‹æŠ½å–å¤±è´¥'}

        # 2. ä¸€è‡´æ€§åˆ†æ
        analysis_result = self.analyze_consistency(model_results)
        entity_consensus = analysis_result['entities']
        relation_consensus = analysis_result.get('relations', {})
        has_relations = analysis_result.get('has_relations', False)

        # 3. è¯†åˆ«å®¡æ ¸å€™é€‰å¹¶ä¿å­˜åˆ°outputs/review_data/human_review_tasks.json
        review_candidates = self.identify_review_candidates(entity_consensus)
        review_tasks_file = None
        if review_candidates:
            with open(text_file, 'r', encoding='utf-8') as f:
                doc_content = f.read()
            review_tasks_file = self.save_review_tasks(review_candidates, doc_name, doc_content)

        # 4. ç”Ÿæˆæœ€ç»ˆå®ä½“å’Œå…³ç³»
        final_entities = self.generate_final_entities(entity_consensus)
        final_relations = self.generate_final_relations(relation_consensus) if has_relations else []

        # 5. ç”ŸæˆBERTè®­ç»ƒæ ¼å¼æ•°æ®
        bert_training_data = self.generate_bert_training_data(text_file, final_entities)

        # 6. æ˜¾ç¤ºç»“æœ
        self.display_results(doc_name, entity_consensus, final_entities, {'reviewed': 0, 'skipped': 0}, final_relations)

        # 7. ä¿å­˜ç»“æœ
        result_files = self.save_results(doc_name, model_results, entity_consensus,
                                       final_entities, {'reviewed': 0, 'skipped': 0}, bert_training_data,
                                       final_relations)

        return {
            'success': True,
            'doc_name': doc_name,
            'final_entities': final_entities,
            'final_relations': final_relations,
            'entity_consensus': entity_consensus,
            'relation_consensus': relation_consensus,
            'bert_training_data': bert_training_data,
            'review_tasks_file': review_tasks_file,
            'result_files': result_files,
            'has_relations': has_relations
        }

    def batch_process(self, max_files: int = None) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ–‡æœ¬æ–‡ä»¶"""

        print("ğŸš€ é’ˆç¸å¤§æˆé›†æˆå®ä½“æŠ½å–ç³»ç»Ÿ")
        print("="*60)
        print("ğŸ¯ åŠŸèƒ½: ä¸‰æ¨¡å‹ååŒæŠ½å– + æ™ºèƒ½å¯¹æ¯” + ç”Ÿæˆå®¡æ ¸ä»»åŠ¡")
        print("ğŸ¤– æ¨¡å‹: è±†åŒ…(æœ€å¯é ) > DeepSeek > é€šä¹‰åƒé—®")
        print("ğŸ”§ è¾“å‡º: BERTè®­ç»ƒæ•°æ® + äººå·¥å®¡æ ¸ä»»åŠ¡")
        print("="*60)

        # è·å–æ–‡æœ¬æ–‡ä»¶
        text_files = self.get_text_files()
        if not text_files:
            return {'success': False, 'error': 'æœªæ‰¾åˆ°æ–‡æœ¬æ–‡ä»¶'}

        if max_files:
            text_files = text_files[:max_files]

        print(f"\nğŸ“‹ å°†å¤„ç† {len(text_files)} ä¸ªæ–‡ä»¶")
        print("ğŸ“ äº‰è®®å®ä½“å°†ä¿å­˜åˆ° outputs/review_data/human_review_tasks.json")
        print("ğŸ¤– BERTè®­ç»ƒæ•°æ®å°†è‡ªåŠ¨ç”Ÿæˆ")

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        all_results = []
        total_entities = 0
        total_review_tasks = 0

        for i, text_file in enumerate(text_files, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(text_files)}: {text_file.name}")
            print(f"{'='*80}")

            try:
                result = self.process_single_file(text_file)

                if result['success']:
                    all_results.append(result)
                    total_entities += len(result['final_entities'])

                    # ç»Ÿè®¡å®¡æ ¸ä»»åŠ¡
                    if result['review_tasks_file'] and os.path.exists(result['review_tasks_file']):
                        with open(result['review_tasks_file'], 'r', encoding='utf-8') as f:
                            review_data = json.load(f)
                            total_review_tasks = review_data.get('total_tasks', 0)

                    print(f"âœ… å¤„ç†å®Œæˆ: {len(result['final_entities'])} ä¸ªæœ€ç»ˆå®ä½“")
                else:
                    print(f"âŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

            except KeyboardInterrupt:
                print(f"\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œå·²å¤„ç† {i-1} ä¸ªæ–‡ä»¶")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
                continue

        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        print(f"\nğŸŠ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   ğŸ“„ å¤„ç†æ–‡ä»¶: {len(all_results)} ä¸ª")
        print(f"   ğŸ“ æœ€ç»ˆå®ä½“: {total_entities} ä¸ª")
        print(f"   ğŸ“‹ å®¡æ ¸ä»»åŠ¡: {total_review_tasks} ä¸ª")
        print(f"   ğŸ¤– BERTè®­ç»ƒæ•°æ®: {len(all_results)} ä¸ªæ–‡ä»¶")

        print(f"\nğŸ”„ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print(f"   1. è¿è¡Œ python human_review_interface.py è¿›è¡Œäººå·¥å®¡æ ¸")
        print(f"   2. ä½¿ç”¨ç”Ÿæˆçš„ outputs/training_data/bert_training_*.json æ–‡ä»¶è®­ç»ƒBERTæ¨¡å‹")

        return {
            'success': True,
            'processed_files': len(all_results),
            'total_entities': total_entities,
            'total_review_tasks': total_review_tasks,
            'results': all_results
        }


def main():
    """ä¸»å‡½æ•°"""

    print("é’ˆç¸å¤§æˆé›†æˆå®ä½“æŠ½å–ç³»ç»Ÿ")
    print("="*50)

    try:
        extractor = IntegratedExtractor()

        # è¯¢é—®æ˜¯å¦æ˜¾ç¤ºprompt
        print("\nè°ƒè¯•é€‰é¡¹:")
        show_prompt = input("æ˜¯å¦æ˜¾ç¤ºæ¨¡å‹Promptåˆ°ç»ˆç«¯ï¼Ÿ(y/N): ").strip().lower()

        # è®¾ç½®å…¨å±€é…ç½®
        try:
            from config import set_show_prompts
            set_show_prompts(show_prompt == 'y')

            if show_prompt == 'y':
                print("âœ… å·²å¯ç”¨Promptæ˜¾ç¤º")
            else:
                print("âŒ å·²ç¦ç”¨Promptæ˜¾ç¤º")
        except ImportError:
            print("âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")

        # è¯¢é—®å¤„ç†æ¨¡å¼
        print("\né€‰æ‹©å¤„ç†æ¨¡å¼:")
        print("1. å•æ–‡ä»¶å¤„ç†")
        print("2. æ‰¹é‡å¤„ç†")
        print("3. ğŸ”— ä¸ºå·²å®Œæˆçš„å®ä½“æŠ½å–å…³ç³»")

        choice = input("è¯·é€‰æ‹© (1-3): ").strip()

        if choice == '1':
            # å•æ–‡ä»¶å¤„ç†
            text_files = extractor.get_text_files()
            if not text_files:
                return

            print(f"\nå¯ç”¨æ–‡ä»¶:")
            for i, file in enumerate(text_files[:10], 1):
                print(f"  {i}. {file.name}")

            file_choice = input(f"é€‰æ‹©æ–‡ä»¶ (1-{min(10, len(text_files))}): ").strip()
            try:
                file_index = int(file_choice) - 1
                if 0 <= file_index < len(text_files):
                    selected_file = text_files[file_index]
                    extractor.process_single_file(selected_file)
                else:
                    print("æ— æ•ˆé€‰æ‹©")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

        elif choice == '2':
            # æ‰¹é‡å¤„ç†
            max_files = input("æœ€å¤§å¤„ç†æ–‡ä»¶æ•° (å›è½¦=å…¨éƒ¨): ").strip()
            max_files = int(max_files) if max_files.isdigit() else None

            extractor.batch_process(max_files=max_files)

        elif choice == '3':
            # å…³ç³»æŠ½å–
            print("\nğŸ”— å¼€å§‹ä¸ºæ‰€æœ‰å·²å®Œæˆçš„å®ä½“æŠ½å–å…³ç³»...")
            print("ğŸ’¡ è¿™å°†åŸºäºå·²ä¿å­˜çš„å®ä½“ç»“æœæ–‡ä»¶è¿›è¡Œå…³ç³»æŠ½å–")

            confirm = input("ç¡®è®¤å¼€å§‹ï¼Ÿ(y/N): ").strip().lower()
            if confirm == 'y':
                result = extractor.extract_relations_for_all_documents()

                if result['success']:
                    print(f"\nğŸ‰ å…³ç³»æŠ½å–å®Œæˆï¼")
                    print(f"ğŸ“Š å¤„ç†æ–‡æ¡£æ•°: {result['total_documents']}")
                    print(f"ğŸ”— æ€»å…³ç³»æ•°: {result['total_relations']}")
                else:
                    print(f"\nâŒ å…³ç³»æŠ½å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                print("å·²å–æ¶ˆ")

        else:
            print("æ— æ•ˆé€‰æ‹©")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
